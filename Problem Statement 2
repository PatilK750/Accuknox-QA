**Objective 1: System Health Monitoring Script**

**Description:** The script monitors CPU usage, memory usage, disk space, and running processes. If any metric exceeds a predefined threshold, it logs an alert.

#### **Python Script:**
```python
import psutil
import logging

# Configure logging
logging.basicConfig(filename='system_health.log', level=logging.INFO, format='%(asctime)s - %(message)s')

# Thresholds
CPU_THRESHOLD = 80  # percent
MEMORY_THRESHOLD = 80  # percent
DISK_THRESHOLD = 80  # percent

def monitor_health():
    # CPU usage
    cpu_usage = psutil.cpu_percent(interval=1)
    if cpu_usage > CPU_THRESHOLD:
        logging.warning(f"High CPU usage: {cpu_usage}%")
        print(f"Alert: CPU usage is {cpu_usage}%")

    # Memory usage
    memory_info = psutil.virtual_memory()
    if memory_info.percent > MEMORY_THRESHOLD:
        logging.warning(f"High Memory usage: {memory_info.percent}%")
        print(f"Alert: Memory usage is {memory_info.percent}%")

    # Disk space
    disk_info = psutil.disk_usage('/')
    if disk_info.percent > DISK_THRESHOLD:
        logging.warning(f"Low Disk Space: {disk_info.percent}% used")
        print(f"Alert: Disk space is {disk_info.percent}% used")

    # Running processes
    process_count = len(psutil.pids())
    logging.info(f"Running processes: {process_count}")
    print(f"System Health - CPU: {cpu_usage}%, Memory: {memory_info.percent}%, Disk: {disk_info.percent}%, Processes: {process_count}")

if __name__ == "__main__":
    monitor_health()
```

#### **How to Run:**
1. Install the required library:
   ```bash
   pip install psutil
   ```
2. Run the script:
   ```bash
   python system_health_monitor.py
   ```
3. Check alerts in the console or view logs in `system_health.log`.

---

### **Objective 2: Automated Backup Solution**

**Description:** The script automates directory backups to a remote server via `scp` (secure copy). It logs success or failure of the backup operation.

#### **Python Script:**
```python
import os
import subprocess
import logging

# Configure logging
logging.basicConfig(filename='backup.log', level=logging.INFO, format='%(asctime)s - %(message)s')

# Source directory and destination
SOURCE_DIR = "/path/to/source"  # Replace with the directory you want to back up
REMOTE_SERVER = "user@remote.server:/path/to/destination"  # Replace with remote server details

def backup_directory():
    try:
        # Construct scp command
        command = f"scp -r {SOURCE_DIR} {REMOTE_SERVER}"
        
        # Execute the command
        result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # Check result
        if result.returncode == 0:
            logging.info("Backup successful.")
            print("Backup completed successfully.")
        else:
            logging.error(f"Backup failed. Error: {result.stderr.decode()}")
            print("Backup failed. Check the log for details.")
    except Exception as e:
        logging.error(f"An exception occurred: {e}")
        print("An error occurred during the backup process.")

if __name__ == "__main__":
    backup_directory()
```

#### **How to Run:**
1. Replace `SOURCE_DIR` and `REMOTE_SERVER` with your directory path and remote server credentials.
2. Ensure you have SSH access set up for the remote server.
3. Run the script:
   ```bash
   python automated_backup.py
   ```

---

### **Objective 3: Log File Analyzer**

**Description:** This script analyzes web server logs and generates a report with metrics like:
- Number of 404 errors.
- Most requested pages.
- Top IPs sending requests.

#### **Python Script:**
```python
import re
from collections import Counter

LOG_FILE = "/path/to/access.log"  # Replace with your web server log file path

def analyze_logs():
    with open(LOG_FILE, 'r') as file:
        logs = file.readlines()

    # Extract patterns
    ip_pattern = r'^(\d+\.\d+\.\d+\.\d+)'  # Extract IP addresses
    page_pattern = r'\"GET (.*?) HTTP'  # Extract requested pages
    status_pattern = r' (\d{3}) '  # Extract HTTP status codes

    ips = []
    pages = []
    status_codes = []

    for log in logs:
        ips.append(re.search(ip_pattern, log).group(1) if re.search(ip_pattern, log) else None)
        pages.append(re.search(page_pattern, log).group(1) if re.search(page_pattern, log) else None)
        status_codes.append(re.search(status_pattern, log).group(1) if re.search(status_pattern, log) else None)

    # Count occurrences
    ip_count = Counter(ips)
    page_count = Counter(pages)
    status_count = Counter(status_codes)

    # Report
    print("Top 5 IP Addresses:")
    for ip, count in ip_count.most_common(5):
        print(f"{ip}: {count} requests")

    print("\nTop 5 Requested Pages:")
    for page, count in page_count.most_common(5):
        print(f"{page}: {count} requests")

    print("\nHTTP Status Code Summary:")
    for code, count in status_count.items():
        print(f"{code}: {count} occurrences")

    print("\n404 Errors:")
    print(f"{status_count.get('404', 0)} occurrences")

if __name__ == "__main__":
    analyze_logs()
```
